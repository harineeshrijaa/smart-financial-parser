Metadata-Version: 2.4
Name: smart_financial_parser
Version: 0.1.0
Summary: Smart Financial Parser
Author: Harinee Saravanakumar
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: python-dateutil
Requires-Dist: dateparser
Requires-Dist: rapidfuzz
Requires-Dist: pytest

# Smart Financial Parser - Harinee Saravanakumar 

A small, test-driven Python CLI that ingests a messy CSV of transactions, normalizes dates/amounts/merchants, categorizes merchants, converts amounts to USD (using deterministic demo FX rates) and produces a top-spending-category report.

This repository is structured for clarity and reproducibility: parsing and canonicalization logic are separated from reporting/FX conversion, and a compact set of unit and integration tests exercise edge cases.

## Quick Start

Clone, create a virtual environment, install dependencies, run tests and the CLI:

```bash
# Clone the repo (creates folder `smart-financial-parser`) and change into it:
git clone https://github.com/harineeshrijaa/smart-financial-parser.git
cd smart-financial-parser

python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# This writes package metadata into the venv so `import smart_financial_parser` works.
pip install -e .

# This is optional — only needed if you enable embedding-based merchant matching.
pip install sentence-transformers

# Run tests
pytest -q

# Produce the top-spending report (reads data/messy_transactions.csv)
python -m smart_financial_parser.cli --input data/messy_transactions.csv --report report/top_spending.json

# Preview cleaned rows (show parsed date and amount columns)
python -m smart_financial_parser.cli --input data/messy_transactions.csv --clean-preview --preview 20
```

## Overview

- Ingests a CSV (flexible quoting and encoding handling).
- Normalizes dates to ISO `YYYY-MM-DD` (`parse_date`).
- Parses amounts into `Decimal` and detects currency where explicit (symbols, 3-letter codes, words).
- Canonicalizes merchants using an alias map + fuzzy matching with `rapidfuzz` and an optional embedding fallback (`sentence-transformers`).
- Categorizes merchants using a small deterministic map and heuristics.
- Aggregates amounts (converted to USD using deterministic demo FX rates) and writes a JSON report with the top spending category.

## Procedure I Took

Below is the development procedure I followed to build the project incrementally and testably:

- Environment & scaffold: created a Python virtualenv, added a minimal `requirements.txt`, and scaffolded the package (`smart_financial_parser/`), `data/`, and `tests/` directories.
- Create messy file: authored `data/messy_transactions.csv` with many realistic, messy examples (mixed date formats, quoted fields, currency symbols, and noisy merchant strings) to drive parsing edge cases. I updated this file throughout the development to account for new edge cases I came up with. 
- Ingest & CLI: implemented `parser/ingest.py` and a small `cli.py` that reads CSV files, supports many flags, and prints user-facing messages for deterministic tests.
- Date & amount normalization: implemented `parse_date` and `parse_amount` in `parser/normalize.py`, adding ordinal stripping, various date format configurations, currency symbol/word detection, and Decimal-based parsing. I extended the functionality by adding more currencies for detection and accounting for edge cases like two-digit years and determining the pivot (25/26). 
- Merchant canonicalization: added `data/merchants.json` and `normalize_merchant` (alias lookup → aggressive-clean → `rapidfuzz` fuzzy match → optional embedding fallback) with `issues` tracking for low-confidence rows. I implementing aggressive-cleaning to account for some words that were not being recognized due to a low fuzzy searching score. 
- Categorization & reporting: implemented `parser/categorize.py` and `parser/report.py` to map merchants to categories, convert amounts to USD (using deterministic demo rates), aggregate totals, and write `report/top_spending.json`.
- CLI integration: added the `--report` flag and hooks to write cleaned CSV and the JSON report; added convenience behaviors (fallback to `data/<basename>`, `--default-currency` preview helper). I added deterministic alphabetical tie-break rules to account for the case of two high spending categories. 
- Tests & iteration: wrote unit tests for parsing, normalization, merchant matching and reporting, plus integration tests that exercise the full pipeline. Ran `pytest -q` frequently and iterated on edge cases exposed by failing tests (CSV quoting, currency symbols, fuzzy thresholds).
- Final polishing: added rounding/formatting helpers, deterministic behavior (alphabetical tiebreak), and README documentation with AI disclosure and methodology notes.

This procedure enforced a test-first, reproducible approach so each change was validated with targeted unit tests before moving on.

## Design Decisions

I researched options and chose pragmatic, well-tested libraries that make the pipeline reliable and easy to test. Here’s why I picked each one and where I use it.

- `pandas`: raw CSV parsing is brittle, so I use `pandas` for resilient reading, preview/sample, and grouping.
  - Where: `parser/ingest.py`, `cli.py` (preview/sample), `parser/report.py` (aggregation).

- `dateparser` + `dateutil`: `dateparser` handles messy human dates; `dateutil` is a solid fallback for tricky inputs. I also strip ordinals and apply an MDY policy with a two-digit-year pivot.
  - Where: `parser/normalize.py::parse_date`.

- `decimal.Decimal`: money needs exact math — `Decimal` prevents floating-point surprises and gives deterministic rounding.
  - Where: `parser/normalize.py::parse_amount`, `convert_amount_to_usd`, and `parser/report.py` when summing/quantizing.

- `rapidfuzz`: fast, deterministic fuzzy matching for merchant aliases. I clean strings aggressively and surface low-confidence matches for review.
  - Where: `parser/merchant.py::normalize_merchant`.

- `sentence-transformers` (optional): embedding fallback for very noisy merchant text — useful but optional due to model downloads, privacy, and cost.
  - Where: optional fallback in `parser/merchant.py` when fuzzy scores are low.

This keeps the codebase small, testable, and extensible — we can plug in live FX or heavier AI components later behind clear interfaces.

## Methodology

### AI Usage Disclosure

I used Microsoft Copilot as an assistant while I developed this project. Copilot helped me quickly clarify Python syntax, suggested prototype code snippets and regexes (for example, ordinal stripping and two-digit-year handling), and provided ideas for parsing heuristics and fuzzy-search tuning.

Concretely, Copilot accelerated these parts of the workflow:
- Prototyping regexes and small parsing helpers that I then turned into unit tests.
- Exploring `pandas`/`dateparser` usage patterns when I wasn’t certain about the exact API surface.
- Helping reason about fuzzy-match thresholds and cleaning steps for merchant canonicalization.

I used Copilot's suggestions as starting points — I accepted, adapted, and rejected suggestions as appropriate. I take full responsibility for every line of code and verified all behavior through tests and manual checks.

### Verification

- Test-first validation: every essential parsing function (`parse_date`, `parse_amount`, `normalize_merchant`) has unit tests that exercise normal cases and edge cases.
- End-to-end checks: the integration tests run the full pipeline on `data/messy_transactions.csv` to validate cleaned output and the produced `report/top_spending.json`.
- Manual verification: I also inspected results on real sample rows by printing cleaned outputs for several representative examples. For example, I outputted the raw dates data from `messy_transactions.csv` against the normalized dates to see if it was translated accurately. I repeated the manual verification for normalized merchants, amounts, and categories. 

Using Copilot saved iteration time, but all parsing heuristics, thresholds, and mappings were validated by tests and manual inspection before being committed.


## Files of interest

- `data/messy_transactions.csv` — sample messy dataset used for integration tests.
- `data/merchants.json` — compact canonical merchant → aliases mapping used for deterministic normalization.
- `smart_financial_parser/cli.py` — Command-Line Interface (flags described below).
- `smart_financial_parser/parser/normalize.py` — date & amount parsing, merchant cleaning helpers, `convert_amount_to_usd`.
- `smart_financial_parser/parser/categorize.py` — merchant → category mapping and heuristics.
- `smart_financial_parser/parser/report.py` — report aggregation, formatting, and JSON writer.
- `tests/` — unit and integration tests (important ones: `test_normalize_date.py`, `test_normalize_amount*.py`, `test_normalize_merchant.py`, `test_report.py`, `test_cli.py`).

## CLI Usage (important flags)

- `--input <path>`: input CSV (required for most actions). If the path doesn't exist, the CLI attempts `data/<basename>` as a convenience fallback.
- `--report <path>`: build and write the top-spending JSON report to the given path.
- `--output <path>`: write cleaned CSV to the given path.
- `--clean-preview`: show a preview including parsed `date_iso`, `amount_decimal`, and `currency` (uses `--preview N` to control rows shown).
- `--preview N`: show N rows (default small number).
- `--sample N`: process only the first N rows (fast iteration).
- `--default-currency CODE`: preview-only helper; when present, missing currencies in the preview are shown as this code (does not mutate data).
- `--verbose`: enables debug logs (stderr) while keeping user-facing messages on stdout for reliable test assertions.

Example:

```bash
python -m smart_financial_parser.cli --input data/messy_transactions.csv --report report/top_spending.json
```

This prints a short summary like `Top spending category: Shopping — $223,417.04` and writes `report/top_spending.json`.

## Report JSON schema

The generated report (`report/top_spending.json`) contains:

- `top_category`: string or `null` — the category with the largest USD total.
- `amount`: formatted top amount (string, e.g., `$1,234.56`) — kept for backward compatibility and human-readability.
- `top_amount`: numeric top amount (float) — programmatic, recommended for downstream processing.
- `by_category`: array of `{ category, amount, pct }` where `amount` is formatted (string) and `pct` is the fraction of total (float).
- `total_usd`: formatted total USD string.

## Normalization & Parsing Notes 

- parse_date(s):
  - Strips ordinal suffixes (`1st`, `2nd`, `3rd`, `4th`).
  - Uses `dateparser` with `DATE_ORDER=MDY`, falls back to `dateutil.parser.parse`.
  - Two-digit-year pivot: current policy maps `00-25` → 2000-2025 and `26-99` → 1900-1999 to reflect the current test pivot behavior (this was tuned during testing). 

- parse_amount(s):
  - Uses `decimal.Decimal` for amounts.
  - Detects and maps currency symbols to codes (expanded to include: `₹, €, £, ¥, Ξ,` and others).
  - Performs word/code detection (e.g., `USD`, `dollars`, `eur`, `pounds`) but only sets a currency when detection is explicit; avoids guessing a currency when input is ambiguous (so `2.00` without a symbol/code will parse numeric amount but leave `currency` as `None`).
  - Handles parentheses and leading-minus for negatives, thousands separators, and malformed numeric strings return `None` for the amount to avoid incorrect assumptions.

- Merchant normalization:
  - Exact alias match → canonical. Otherwise, `rapidfuzz` fuzzy match against canonical aliases.
  - Aggressive cleaning (strip punctuation, collapse whitespace) is applied before fuzzy scoring to improve match confidence.
  - Low-confidence matches are flagged in an `issues` column and an optional embedding-based fallback (using `sentence-transformers`) can be enabled to re-score ambiguous cases.

## Tests & Acceptance

- Run the full test suite:
```bash
pytest -q
```

- Key tests:
  - `tests/test_normalize_date.py` — date parsing edge cases (ordinals, two-digit years, empty/None inputs, MDY policy).
  - `tests/test_normalize_amount.py` — amount parsing, currency detection, separators, negatives.
  - `tests/test_normalize_merchant.py` — canonicalization and fuzzy match behavior.
  - `tests/test_report.py` & integration tests — end-to-end report generation and CLI behavior.


## Limitations & Future Improvements

Below are three concrete improvements I would make next, with short explanations of why they matter and why I didn't implement them for this submission.

- Live FX rates via a trusted API
  - What I'd change: replace the fixed demo FX table with a small adapter that fetches live exchange rates from a trusted provider (or accepts a `--rates-file` JSON input) and applies caching and daylight-stable caching policies.
  - Why it matters: real exchange rates fluctuate and accurate USD conversions are essential for production reporting and auditing.
  - Why I didn't implement it now: integrating a live FX API requires handling authentication, rate limits, error/retry logic, caching for reproducible runs, and tests that remain deterministic. With only two days for this challenge I prioritized a deterministic demo table so tests stay reproducible and reviewers can verify results consistently.

- Expand the alias map with LLM/embedding assistance
  - What I'd change: augment the compact `data/merchants.json` alias map by generating candidate aliases and similarity signals with an embedding model or LLM, then provide a human-in-the-loop review UI to accept/reject new aliases.
  - Why it matters: fuzzy matching has limits on very noisy or truncated merchant strings; embeddings/LLMs can surface non-obvious aliases and increase match coverage and confidence.
  - Why I didn't implement it now: embedding/LLM approaches introduce security/privacy considerations (merchant/transaction data potentially sent to third-party services), pricing and quota concerns for model usage, and the need to build a safe review workflow. Those topics require more design and time than available in this short deadline.

- Improve category mapping with API/AI assistance
  - What I'd change: use an API or AI-assisted mapping step to suggest categories for canonical merchants (and provide confidence scores), then fall back to the deterministic map + heuristics for low-confidence cases.
  - Why it matters: manual category maps are compact and fast, but AI-assisted suggestions speed coverage and help surface ambiguous cases that need human review.
  - Why I didn't implement it now: an AI-assisted categorization pipeline needs a validation and override process to avoid misclassification; building that safely and testably (and verifying the suggestions) was outside the two-day scope.

These three improvements are deliberate next steps for a production-ready pipeline. I would like to integrate them, and I researched and applied mature Python libraries and deterministic techniques (pandas, dateparser, rapidfuzz, Decimal, etc.) to achieve reliable results within the short deadline. That approach delivered a small, auditable, and testable baseline while leaving the architecture open so live FX, embedding/LLM alias expansion, or AI-assisted categorization can be added safely in a follow-up.

## Final notes / Developer rationale

This project was built under a tight, test-driven constraint: prioritize correctness and reproducibility. I used small, deterministic datasets and fixed demo FX rates so that behavior is repeatable in tests. AI-assisted suggestions helped speed iteration on regexes and edge-case thinking but I validated and adapted those suggestions; I accept responsibility for the final implementation and tests.


